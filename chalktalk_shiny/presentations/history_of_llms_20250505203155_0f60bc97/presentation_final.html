<!DOCTYPE html>
<html lang="en">
 <head>
  <script src="presentation_files/libs/clipboard/clipboard.min.js">
  </script>
  <script src="presentation_files/libs/quarto-html/tabby.min.js">
  </script>
  <script src="presentation_files/libs/quarto-html/popper.min.js">
  </script>
  <script src="presentation_files/libs/quarto-html/tippy.umd.min.js">
  </script>
  <link href="presentation_files/libs/quarto-html/tippy.css" rel="stylesheet"/>
  <link href="presentation_files/libs/quarto-html/light-border.css" rel="stylesheet"/>
  <link data-mode="light" href="presentation_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet"/>
  <link href="presentation_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
  <meta charset="utf-8"/>
  <meta content="quarto-1.5.56" name="generator"/>
  <title>
   chalktalk Demo
  </title>
  <meta content="yes" name="apple-mobile-web-app-capable"/>
  <meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"/>
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui" name="viewport"/>
  <link href="presentation_files/libs/revealjs/dist/reset.css" rel="stylesheet"/>
  <link href="presentation_files/libs/revealjs/dist/reveal.css" rel="stylesheet"/>
  <style>
   code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link href="presentation_files/libs/revealjs/dist/theme/quarto.css" rel="stylesheet"/>
  <link href="presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet"/>
  <link href="presentation_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet"/>
  <link href="presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet"/>
  <link href="presentation_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet"/>
  <style type="text/css">
   .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }
  </style>
  <style type="text/css">
   .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
 </head>
 <body class="quarto-dark">
  <div class="reveal">
   <div class="slides">
    <section class="quarto-title-block center" data-autoslide="0" id="title-slide">
     <h1 class="title">
      chalktalk Demo
     </h1>
     <div class="quarto-title-authors">
     </div>
     <button id="startPresentationButton" style="position: absolute; z-index: 1000; bottom: 20px; left: 50%; transform: translateX(-50%);">
      Start Presentation
     </button>
    </section>
    <section data-autoslide="100">
     <div class="fragment" data-autoslide="10">
     </div>
     <section class="title-slide slide level1 center" data-autoslide="100" id="history-of-llms">
      <div class="fragment" data-autoslide="10">
      </div>
      <h1>
       History of LLMs
      </h1>
     </section>
     <section class="slide level2" data-autoslide="100" id="from-n-grams-to-chatgpt-a-very-brief-history-of-language-modeling">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       From n-grams to ChatGPT: A Very Brief History of Language Modeling
      </h2>
     </section>
     <section class="slide level2" data-autoslide="100" id="why-the-past-matters-13">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Why the Past Matters (1/3)
      </h2>
      <div class="fragment" data-autoslide="10548" data-tts="Why delve into the history of language models, especially for life scientists? Because understanding the past helps us understand the present and anticipate the future." data-tts-id="tts_0_1c9b35c7">
       <audio data-autoplay="">
        <source src="media/audio/Why_delve_into_the_h_0730039b-9d64-404b-b6e9-d6f0c07fb1c5.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="10044" data-tts="First, technical capabilities are compounding rapidly. Progress isn't linear; it's accelerating. Think about Moore's Law, but for language AI." data-tts-id="tts_1_4d4ff6cc">
       <ul>
        <li>
         Technical capabilities are
         <strong>
          compounding
         </strong>
         &rarr; forecasting requires a historical lens
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/First_technical_capa_c2b06646-3d87-4507-a879-1b3e74cef95e.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="14040" data-tts="Consider this: GPT-1 is only about seven years old as of this writing. The pace is truly picking up. Forecasting even the next two years is incredibly challenging without this historical context." data-tts-id="tts_2_abb87e7e">
       <audio data-autoplay="">
        <source src="media/audio/Consider_this_GPT1_i_2a708f7c-319a-46d0-ab42-be1896d4a3e1.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <aside class="notes">
       <p>
        Note that GPT-1 is only
        <em>
         seven
        </em>
        years old; the pace is
        <strong>
         accelerating
        </strong>
        .
       </p>
       <p>
        Forecasting the next two years is challenging without this backstory.
       </p>
       <style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }
       </style>
      </aside>
     </section>
     <section class="slide level2" data-autoslide="100" id="why-the-past-matters-23">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Why the Past Matters (2/3)
      </h2>
      <div class="fragment" data-autoslide="13536" data-tts="Second, the pitfalls we encounter today &ndash; things like bias in the model's responses, the tendency to 'hallucinate' or make things up, and the sheer cost of training and running these models &ndash; aren't random bugs." data-tts-id="tts_3_d190198c">
       <audio data-autoplay="">
        <source src="media/audio/Second_the_pitfalls__13dc2a0d-6dc6-4d4a-b0ba-64b39fce7013.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="9540" data-tts="They often trace straight back to specific decisions made during the model's training process, the data it was fed, and the objectives it was optimized for." data-tts-id="tts_4_16d931e9">
       <ul>
        <li>
         Pitfalls (bias, hallucination, cost) trace straight back to each training step
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/They_often_trace_str_9f824740-d48b-4f27-ab29-2977f75651d0.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="why-the-past-matters-33">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Why the Past Matters (3/3)
      </h2>
      <div class="fragment" data-autoslide="5112" data-tts="So, our goal today isn't just to give you a recipe for using these tools." data-tts-id="tts_5_1dd29702">
       <audio data-autoplay="">
        <source src="media/audio/So_our_goal_today_is_f2d0b4c4-d5b1-455d-ba3a-92b5d5c8bd3b.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="10512" data-tts="It's to provide a mental model &ndash; a foundational understanding of how these models evolved, why they work the way they do, and what their inherent strengths and limitations are." data-tts-id="tts_6_3b9947b6">
       <ul>
        <li>
         Goal today: give life-science graduates a
         <strong>
          mental model
         </strong>
         , not just a recipe
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Its_to_provide_a_men_a49e633e-eb60-4c07-91c8-d48c0bcdacd7.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="what-is-a-language-model">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       What is a language model?
      </h2>
      <div class="fragment" data-autoslide="5544" data-tts="Before we dive into history, let's define our terms. What *is* a language model at its core?" data-tts-id="tts_7_20c131aa">
       <ul>
        <li>
         <strong>
          Definition:
         </strong>
         A language model is a type of AI that has been trained to understand and generate human language.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Before_we_dive_into__4381e4f4-7d10-4f20-84d9-9f1570e3dc43.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="10692" data-tts="Fundamentally, it learns the probability of sequences of words occurring. Given some text, its basic task is often to predict the *next* word or token." data-tts-id="tts_8_14b4fcb3">
       <ul>
        <li>
         It learns patterns, grammar, and even some degree of &lsquo;knowledge&rsquo; from the vast amounts of text data it&rsquo;s trained on.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Fundamentally_it_lea_32486b3d-4b60-4f18-84e4-db46325347ba.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="9684" data-tts="Think of it like a super-powered autocomplete, but one that can handle much more complex tasks like translation, summarization, and answering questions." data-tts-id="tts_9_71f489b6">
       <audio data-autoplay="">
        <source src="media/audio/Think_of_it_like_a_s_925a02b8-57da-4aa5-86df-f50e88aa9053.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="one-page-lineage-table-roadmap">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       One-page lineage table (Roadmap)
      </h2>
      <div class="fragment" data-autoslide="6948" data-tts="Here's a summary table outlining the key steps in model evolution we'll discuss today. Think of this as our roadmap." data-tts-id="tts_10_76e99319">
       <p>
        <em>
         A summary of the key steps in model evolution we&rsquo;ll discuss.
        </em>
       </p>
       <audio data-autoplay="">
        <source src="media/audio/Heres_a_summary_tabl_967bb944-56f3-40b0-87d3-db39d80671ac.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="10584" data-tts="We'll start with simple N-grams from the 1960s and walk through the major innovations leading up to today's sophisticated models like GPT-4o1." data-tts-id="tts_11_18bcf032">
       <table class="caption-top">
        <colgroup>
         <col style="width: 10%"/>
         <col style="width: 21%"/>
         <col style="width: 12%"/>
         <col style="width: 6%"/>
         <col style="width: 9%"/>
         <col style="width: 30%"/>
         <col style="width: 7%"/>
        </colgroup>
        <thead>
         <tr class="header">
          <th>
           Model (year)
          </th>
          <th>
           Incremental training steps
          </th>
          <th>
           Corpus size (tokens &asymp; &ldquo;Bibles&rdquo;*)
          </th>
          <th>
           Max context (tokens)
          </th>
          <th>
           Rough train compute
          </th>
          <th>
           Typical answer to &ldquo;Who is the current pope? (3 May 2025)&rdquo;
          </th>
          <th>
           Typical inference HW
          </th>
         </tr>
        </thead>
        <tbody>
         <tr class="odd">
          <td>
           <strong>
            4-gram (Brown, 1960s)
           </strong>
          </td>
          <td>
           Count 4-grams, MLE
          </td>
          <td>
           1 M &asymp; 1.3 Bibles
          </td>
          <td>
           3 (last three words)
          </td>
          <td>
           &lt; 1 CPU-h
          </td>
          <td>
           &ldquo;the pope is the &hellip;&rdquo; (nonsense)
          </td>
          <td>
           any laptop CPU
          </td>
         </tr>
         <tr class="even">
          <td>
           <strong>
            GPT-1 &rarr; GPT-3 (2018-20)
           </strong>
          </td>
          <td>
           Same next-token objective,
           <strong>
            massive scale-up
           </strong>
           of params &amp; data
          </td>
          <td>
           0.8 B &rarr; 500 B &asymp; 1 K &rarr; 640 K Bibles &uarr;
          </td>
          <td>
           512 &rarr; 2 048 &uarr;
          </td>
          <td>
           &asymp;3 &times; 10&sup2;&sup3; FLOP &asymp; 355 V100-years &uarr;
          </td>
          <td>
           &ldquo;Read our new article on the pope here.&rdquo; (fluent but ignores the question&rsquo;s intent)
          </td>
          <td>
           single GPU &rarr; multi-GPU
          </td>
         </tr>
         <tr class="odd">
          <td>
           <strong>
            GPT-3.5 / ChatGPT (2022)
           </strong>
          </td>
          <td>
           + Supervised instruction fine-tune + RLHF
          </td>
          <td>
           &asymp; 500 B + few &times; 10‚Åµ labelled prompts
          </td>
          <td>
           4 096 &uarr;
          </td>
          <td>
           + a few V100-days &uarr;
          </td>
          <td>
           &ldquo;Pope Francis.&rdquo; (Model follows instruction but is
           <strong>
            stale
           </strong>
           ; unaware of April 2025 death.)
          </td>
          <td>
           cloud GPU
          </td>
         </tr>
         <tr class="even">
          <td>
           <strong>
            GPT-4o / GPT-4 Turbo (23-24)
           </strong>
          </td>
          <td>
           + Tool-use fine-tune (plugins: Browse, Code Interpreter)
          </td>
          <td>
           &asymp; 1 T &asymp; 1.6 M Bibles &uarr;
          </td>
          <td>
           8 K / 128 K &uarr;
          </td>
          <td>
           &asymp; 2.5 M A100-days &uarr;
          </td>
          <td>
           &ldquo;Pope Francis died on 14 Apr 2025; Pope John XXIV was elected on 28 Apr 2025.&rdquo; (cites news)
          </td>
          <td>
           cloud GPU + API calls
          </td>
         </tr>
         <tr class="odd">
          <td>
           <strong>
            GPT-4o1 (reasoning, 24-25)
           </strong>
          </td>
          <td>
           + Chain-of-thought reinforcement (post-training)
          </td>
          <td>
           same &asymp; 1 T
          </td>
          <td>
           8 K &ndash; 128 K
          </td>
          <td>
           + &asymp; 10&ndash;20 k A100-days &uarr;
          </td>
          <td>
           Gives same facts
           <strong>
            and
           </strong>
           step-by-step justification (citing sources).
          </td>
          <td>
           cloud GPU + tools
          </td>
         </tr>
        </tbody>
       </table>
       <audio data-autoplay="">
        <source src="media/audio/Well_start_with_simp_cfdddc75-b4fc-4fbe-9072-bd34676e0199.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="11808" data-tts="We'll examine each row, explaining the incremental changes and their consequences, focusing on how each step built upon the last. Then we'll return here for a final summary." data-tts-id="tts_12_70ffe61b">
       <p>
        <em>
         Bible rough equivalence uses &asymp; 780,000 words per King James Bible. Pope example is hypothetical.
        </em>
       </p>
       <audio data-autoplay="">
        <source src="media/audio/Well_examine_each_ro_1b1532f5-8046-44e7-9cae-44c74056e2cf.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <aside class="notes">
       <p>
        Introduce this as the roadmap. We&rsquo;ll walk through each row, explaining the
        <em>
         incremental
        </em>
        changes and their consequences, then return here for a summary.
       </p>
       <style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }
       </style>
      </aside>
     </section>
     <section class="slide level2" data-autoslide="100" id="baseline-n-gram-language-models-12">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Baseline: N-gram Language Models (1/2)
      </h2>
      <div class="fragment" data-autoslide="6300" data-tts="Let's begin at the beginning, with N-gram models, which were foundational in computational linguistics." data-tts-id="tts_13_e5355a22">
       <audio data-autoplay="">
        <source src="media/audio/Lets_begin_at_the_be_765e218b-6b14-4544-8196-e133723d30c2.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="11340" data-tts="The core idea is simple: Predict the next word based *only* on the previous 'N minus one' words. For a 4-gram model, that means looking at just the last three words." data-tts-id="tts_14_acd9a690">
       <ul>
        <li>
         <strong>
          Core Idea:
         </strong>
         Predict the next word based
         <em>
          only
         </em>
         on the previous N-1 words.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/The_core_idea_is_sim_10a6d496-3507-4d31-8451-0d732b6ba2f1.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="11664" data-tts="Training these models involves simply counting sequences of N words in a text corpus. This statistical approach is called Maximum Likelihood Estimation, or MLE." data-tts-id="tts_15_d15b6b0e">
       <ul>
        <li>
         <strong>
          Training:
         </strong>
         Simply count sequences of N words in a text corpus (Maximum Likelihood Estimation - MLE).
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Training_these_model_7efa189d-1263-457b-9806-90a422c650b0.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="baseline-n-gram-language-models-22">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Baseline: N-gram Language Models (2/2)
      </h2>
      <div class="fragment" data-autoslide="17100" data-tts="The major limitation is obvious: N-grams understand only very local word patterns. They have no concept of grammar beyond the N-word window, no broader context, no world knowledge, and certainly no understanding of user instructions or questions." data-tts-id="tts_16_8e9286bc">
       <ul>
        <li>
         <strong>
          Limitation:
         </strong>
         Understands local word patterns but has no broader context, world knowledge, or concept of instructions/questions.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/The_major_limitation_ed2d6c93-b594-4ee0-9160-6e0cd2d986aa.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="8640" data-tts="Here's the N-gram row from our table. Notice the tiny context, minimal compute, and nonsensical output for our test question." data-tts-id="tts_17_f78677a6">
       <table class="caption-top">
        <colgroup>
         <col style="width: 12%"/>
         <col style="width: 12%"/>
         <col style="width: 15%"/>
         <col style="width: 9%"/>
         <col style="width: 9%"/>
         <col style="width: 28%"/>
         <col style="width: 9%"/>
        </colgroup>
        <thead>
         <tr class="header">
          <th>
           Model (year)
          </th>
          <th>
           Incremental training steps
          </th>
          <th>
           Corpus size (tokens &asymp; &ldquo;Bibles&rdquo;*)
          </th>
          <th>
           Max context (tokens)
          </th>
          <th>
           Rough train compute
          </th>
          <th>
           Typical answer to &ldquo;Who is the current pope? (3 May 2025)&rdquo;
          </th>
          <th>
           Typical inference HW
          </th>
         </tr>
        </thead>
        <tbody>
         <tr class="odd">
          <td>
           <strong>
            4-gram (Brown, 1960s)
           </strong>
          </td>
          <td>
           Count 4-grams, MLE
          </td>
          <td>
           1 M &asymp; 1.3 Bibles
          </td>
          <td>
           3 (last three words)
          </td>
          <td>
           &lt; 1 CPU-h
          </td>
          <td>
           &ldquo;the pope is the &hellip;&rdquo; (nonsense)
          </td>
          <td>
           any laptop CPU
          </td>
         </tr>
        </tbody>
       </table>
       <audio data-autoplay="">
        <source src="media/audio/Heres_the_Ngram_row__1536fdc2-57b6-4854-a3f3-eb23c6c25a09.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="n-grams-small-but-kinda-useful">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       N-grams: Small but kinda useful
      </h2>
      <div class="fragment" data-autoslide="8712" data-tts="Now, I'll switch to a quick demo to show what typical output from a trigram or four-gram model looks like. Expect statistical word salad!" data-tts-id="tts_18_b494e661">
       <p>
        <em>
         (Note: We&rsquo;ll switch to a live demo here to show trigram/4-gram output.)
        </em>
       </p>
       <audio data-autoplay="">
        <source src="media/audio/Now_Ill_switch_to_a__b22f579e-6c17-4485-ad6a-f95a1f279fa6.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="8532" data-tts="Despite their limitations, N-grams weren't useless! Their underlying principle powers simple applications we still see." data-tts-id="tts_19_33a2ff1c">
       <audio data-autoplay="">
        <source src="media/audio/Despite_their_limita_dad427d1-4df2-48b6-beda-29550f7cfe1b.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="12492" data-tts="Think about the predictive text on your phone or the query suggestions in Google search. These often use N-gram-like techniques to guess the next few characters or words you're likely to type based on frequency." data-tts-id="tts_20_c3796dbf">
       <ul>
        <li>
         Examples: Early machine translation, spell checkers, mobile keyboard prediction, search query suggestion.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Think_about_the_pred_4d0836e5-ddbf-4348-a407-435e074c986f.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="5580" data-tts="But for complex understanding or generation, we needed something much smarter." data-tts-id="tts_21_b14a12f4">
       <p>
        <em>
         (Image of Google search prediction could go here)
        </em>
       </p>
       <audio data-autoplay="">
        <source src="media/audio/But_for_complex_unde_639cccad-4342-44ad-9e5f-372b9d98401a.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="need-a-smarter-way-enter-the-transformer-12">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Need a Smarter Way: Enter the Transformer (1/2)
      </h2>
      <div class="fragment" data-autoslide="8100" data-tts="The big leap forward came with the Transformer architecture, introduced in the 2017 paper 'Attention Is All You Need'." data-tts-id="tts_22_b6232685">
       <ul>
        <li>
         <strong>
          Key Innovation:
         </strong>
         The Transformer Architecture (Vaswani et al., 2017)
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/The_big_leap_forward_d173cc1b-83fd-4c11-9c5d-5363b20d01fc.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="18180" data-tts="The Transformer provided a much more sophisticated way to model language, particularly dependencies between words far apart in a sentence or document. It still aimed to predict the next token, but used a mechanism called 'attention' to weigh the importance of different input words." data-tts-id="tts_23_9998ea7d">
       <ul>
        <li>
         A more intelligent&hellip; and
         <strong>
          much more parallelizable
         </strong>
         &hellip; way to do next token prediction.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/The_Transformer_prov_8051e01e-3230-4c20-aadc-c682b48c37e8.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="8748" data-tts="Crucially, its design was highly parallelizable, meaning it could be trained efficiently on massive datasets using modern GPUs." data-tts-id="tts_24_9d2b6343">
       <ul>
        <li>
         <strong>
          Approach:
         </strong>
         Keep the same core objective (predict the next token) but use a better architecture.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Crucially_its_design_af2c17ab-5052-426d-b5c5-15170b0d7716.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="need-a-smarter-way-enter-the-transformer-22">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Need a Smarter Way: Enter the Transformer (2/2)
      </h2>
      <div class="fragment" data-autoslide="16488" data-tts="Here's a very high-level schematic. The key idea is that 'attention mechanisms' allow the model to look at all parts of the input sequence simultaneously and decide which parts are most relevant for predicting the next word, unlike older recurrent models that processed words one by one." data-tts-id="tts_25_6842375a">
       <p>
        [IMAGE showing simplified Transformer architecture with attention]
       </p>
       <audio data-autoplay="">
        <source src="media/audio/Heres_a_very_highlev_ae756e63-587c-407b-a5b5-d13778505a60.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="6048" data-tts="This ability to handle long-range dependencies and its parallelizability were game-changers." data-tts-id="tts_26_0140b8c3">
       <audio data-autoplay="">
        <source src="media/audio/This_ability_to_hand_5223427f-69ab-4df4-9041-9920d71cc24a.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="what-does-gpt-stand-for">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       What does GPT stand for?
      </h2>
      <div class="fragment" data-autoslide="7632" data-tts="This brings us to the GPT series from OpenAI. GPT stands for Generative Pre-trained Transformer." data-tts-id="tts_27_98a93f27">
       <ul>
        <li>
         <strong>
          G
         </strong>
         enerative: It can generate new text.
        </li>
        <li>
         <strong>
          P
         </strong>
         re-trained: It&rsquo;s trained on a massive dataset
         <em>
          before
         </em>
         being fine-tuned for specific tasks.
        </li>
        <li>
         <strong>
          T
         </strong>
         ransformer: It uses the Transformer architecture.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/This_brings_us_to_th_f9850c34-5122-4219-9999-53dd9c9c414f.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="8640" data-tts="The core idea of GPT models, especially early ones, was to take the Transformer architecture and scale it up dramatically." data-tts-id="tts_28_a0e4ed89">
       <audio data-autoplay="">
        <source src="media/audio/The_core_idea_of_GPT_7d72680d-c38b-4820-a31e-060835718cdc.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="the-training-data-fuel-for-the-transformer">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       The Training Data: Fuel for the Transformer
      </h2>
      <div class="fragment" data-tts="Because the Transformer architecture is so effective at learning patterns and parallelizable for training, researchers realized they could train these 'Generative Pre-trained' models on truly enormous amounts of text data." data-tts-id="tts_29_d2f1a4a9">
       <ul>
        <li>
         Transformer&rsquo;s effectiveness + parallelizability &rArr; Train on
         <strong>
          massive
         </strong>
         datasets.
        </li>
       </ul>
      </div>
      <div class="fragment" data-autoslide="13716" data-tts="Where does this data come from? Essentially, large swathes of the public internet: websites, books, articles, code repositories, and more. Common Crawl, a publicly available web scrape, is a major source." data-tts-id="tts_30_4d822d1f">
       <ul>
        <li>
         Source: Web scrapes (like Common Crawl), books, articles, code, etc.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Where_does_this_data_882ac251-86cf-4363-9e9f-beee7100d1ab.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="16776" data-tts="This reliance on vast, unfiltered internet data is powerful but also introduces challenges, like inheriting biases present online. You might have seen news about lawsuits, like the one from the New York Times, concerning the use of copyrighted material in training data." data-tts-id="tts_31_7ec13759">
       <ul>
        <li>
         Implications: Scale enables powerful models, but also ingests bias and raises copyright questions (e.g., NYT lawsuit).
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/This_reliance_on_vas_511a86c8-a988-41ba-b979-8ae94a67bc62.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="gpt-1-gpt-3-scaling-the-transformer-12">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       GPT-1 &rarr; GPT-3: Scaling the Transformer (1/2)
      </h2>
      <div class="fragment" data-autoslide="15264" data-tts="The period from 2018 to 2020 saw the rapid evolution from GPT-1 to GPT-3. The core recipe didn't change drastically: it was still about predicting the next token using the Transformer architecture." data-tts-id="tts_32_e3e892d2">
       <audio data-autoplay="">
        <source src="media/audio/The_period_from_2018_4c6ae63e-a8ad-4366-8e00-103605c356df.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="13212" data-tts="The key difference was **scale**: massively increasing the number of parameters in the model (from ~117M in GPT-1 to 175B in GPT-3) and the size of the training dataset." data-tts-id="tts_33_9fd69154">
       <ul>
        <li>
         <strong>
          Incremental Step:
         </strong>
         Same next-token objective,
         <strong>
          massive scale-up
         </strong>
         of parameters &amp; data.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/The_key_difference_w_5b513a7d-d6f7-4653-8393-0c07f43f53dc.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="2520" data-tts="Let's look at the table row for this era." data-tts-id="tts_34_eeae13dd">
       <table class="caption-top">
        <colgroup>
         <col style="width: 10%"/>
         <col style="width: 23%"/>
         <col style="width: 14%"/>
         <col style="width: 7%"/>
         <col style="width: 10%"/>
         <col style="width: 25%"/>
         <col style="width: 8%"/>
        </colgroup>
        <thead>
         <tr class="header">
          <th>
           Model (year)
          </th>
          <th>
           Incremental training steps
          </th>
          <th>
           Corpus size (tokens &asymp; &ldquo;Bibles&rdquo;*)
          </th>
          <th>
           Max context (tokens)
          </th>
          <th>
           Rough train compute
          </th>
          <th>
           Typical answer to &ldquo;Who is the current pope? (3 May 2025)&rdquo;
          </th>
          <th>
           Typical inference HW
          </th>
         </tr>
        </thead>
        <tbody>
         <tr class="odd">
          <td>
           <strong>
            GPT-1 &rarr; GPT-3 (2018-20)
           </strong>
          </td>
          <td>
           Same next-token objective,
           <strong>
            massive scale-up
           </strong>
           of params &amp; data
          </td>
          <td>
           0.8 B &rarr; 500 B &asymp; 1 K &rarr; 640 K Bibles &uarr;
          </td>
          <td>
           512 &rarr; 2 048 &uarr;
          </td>
          <td>
           &asymp;3 &times; 10&sup2;&sup3; FLOP &asymp; 355 V100-years &uarr;
          </td>
          <td>
           &ldquo;Read our new article on the pope here.&rdquo; (fluent but ignores intent)
          </td>
          <td>
           single GPU &rarr; multi-GPU
          </td>
         </tr>
        </tbody>
       </table>
       <audio data-autoplay="">
        <source src="media/audio/Lets_look_at_the_tab_482c7b99-07dd-4792-9134-8adf3c162d78.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="gpt-1-gpt-3-scaling-the-transformer-22">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       GPT-1 &rarr; GPT-3: Scaling the Transformer (2/2)
      </h2>
      <div class="fragment" data-autoslide="11340" data-tts="Notice the dramatic increase in corpus size, context length, and especially training compute. The 'Bible equivalence' jumps from around a thousand to over half a million!" data-tts-id="tts_35_f68b947f">
       <ul>
        <li>
         Corpus size: 0.8 B &rarr; ~500 B tokens (&uarr; 600x)
        </li>
        <li>
         Context length: 512 &rarr; 2048 tokens (&uarr; 4x)
        </li>
        <li>
         Compute: Significant increase (measured in GPU-years)
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Notice_the_dramatic__64c4c8c7-2a51-4e1f-ad98-7455ac8ab47c.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="14112" data-tts="What did this scaling achieve? These 'base models' became incredibly fluent and knowledgeable, capable of generating remarkably human-like text. However, they weren't necessarily helpful or aligned with user intent." data-tts-id="tts_36_ca871c3d">
       <audio data-autoplay="">
        <source src="media/audio/What_did_this_scalin_eb667805-e1e3-4e59-87ca-68545b09bb4b.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="13500" data-tts="If you asked GPT-3 our pope question, it might give a fluent but evasive answer, like suggesting an article, essentially completing the prompt based on patterns in its training data rather than directly answering the question." data-tts-id="tts_37_30806212">
       <ul>
        <li>
         Result: Fluent, knowledgeable base models, but not inherently conversational or instruction-following.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/If_you_asked_GPT3_ou_5adf61be-ae00-4351-a462-d738f16ee2c7.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="14472" data-tts="Let me show you an example of how a base GPT model might respond... something like 'The current pope is discussed in several recent theological journals. One article explores...'. Notice the fluency, but it doesn't answer the question." data-tts-id="tts_38_1b1fabdc">
       <p>
        <em>
         (Note: Live demo of a GPT-2/3 base model answer:
        </em>
        &rdquo;The current pope is discussed in several recent theological journals. One article explores&hellip;&ldquo;* &ndash; shows fluency but avoidance.)*
       </p>
       <audio data-autoplay="">
        <source src="media/audio/Let_me_show_you_an_e_5ab48704-13d4-4634-8dcf-a03b00356de1.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="but-can-the-model-chat-aligning-models">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       But can the model chat? Aligning Models
      </h2>
      <div class="fragment" data-autoslide="10440" data-tts="So, these large base models were powerful text predictors, but they weren't chatbots. They didn't inherently understand the *intent* behind a question or instruction." data-tts-id="tts_39_a0ff46fd">
       <ul>
        <li>
         Problem: Base models predict text, don&rsquo;t necessarily follow instructions or converse helpfully.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/So_these_large_base__38db2c95-c577-4e9d-80bb-c91770493700.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="9648" data-tts="How do you teach a model to be helpful, honest, and harmless? The key was 'alignment' &ndash; fine-tuning the pre-trained model to behave in desired ways." data-tts-id="tts_40_2c3fa9fe">
       <ul>
        <li>
         Solution: Post-training &ldquo;Alignment&rdquo; phase.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/How_do_you_teach_a_m_e13ffe68-2b70-4f36-8608-85fef3bf9082.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="7956" data-tts="If we show the model examples of good conversations, like question-and-answer pairs, perhaps it can learn to respond more appropriately." data-tts-id="tts_41_0f90dcf4">
       <ul>
        <li>
         Idea: Fine-tune on chat Q&amp;As and preferred responses.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/If_we_show_the_model_f5e9480c-2a09-4cb2-8c10-2297392e11a8.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="gpt-3.5-chatgpt-aligning-with-user-intent-12">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       GPT-3.5 / ChatGPT: Aligning with User Intent (1/2)
      </h2>
      <div class="fragment" data-tts="This alignment process became the hallmark of models like InstructGPT and, most famously, ChatGPT (often referred to as GPT-3.5). It involved two main steps after the initial pre-training." data-tts-id="tts_42_39c3b3cf">
       <ul>
        <li>
         <strong>
          Key Innovation:
         </strong>
         Post-training &ldquo;Alignment&rdquo; to make the base model more helpful and follow instructions.
        </li>
       </ul>
      </div>
      <div class="fragment" data-autoslide="12600" data-tts="First, Supervised Fine-Tuning or SFT. Human labelers wrote examples of prompts and ideal answers, essentially showing the model how it *should* respond to various instructions and questions." data-tts-id="tts_43_0228c2bf">
       <pre><code>*   **Supervised Fine-Tuning (SFT):** Train on examples of desired input/output pairs (e.g., Q&amp;A format).</code></pre>
       <audio data-autoplay="">
        <source src="media/audio/First_Supervised_Fin_73454f5d-b065-4ba7-9496-db9e56bed3ac.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="22644" data-tts="Second, Reinforcement Learning from Human Feedback or RLHF. Here, the model generated multiple responses to a prompt, and human labelers ranked them from best to worst. This preference data was used to train a 'reward model', which then guided the main LLM (via reinforcement learning) to produce outputs similar to the highly-ranked ones." data-tts-id="tts_44_2225070b">
       <pre><code>*   **Reinforcement Learning from Human Feedback (RLHF):** Use human preferences to rank model outputs, training a reward model to guide the LLM towards helpful, harmless, and honest responses.</code></pre>
       <audio data-autoplay="">
        <source src="media/audio/Second_Reinforcement_90a093a3-bd8f-4c06-ba88-ce01bb7ec2bd.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="gpt-3.5-chatgpt-aligning-with-user-intent-22">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       GPT-3.5 / ChatGPT: Aligning with User Intent (2/2)
      </h2>
      <div class="fragment" data-autoslide="11880" data-tts="The result? A model that understands it should answer questions directly, engage in conversation, admit limitations, and generally adhere to safety guidelines against harmful content." data-tts-id="tts_45_d526e38c">
       <ul>
        <li>
         <strong>
          Result:
         </strong>
         Model now understands it should
         <em>
          answer
         </em>
         questions, be conversational, and adhere to safety guidelines.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/The_result_A_model_t_e3a97279-cb2a-4c10-bf8b-31a01517f86d.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="15372" data-tts="Here's the table entry for GPT-3.5/ChatGPT. Notice the alignment steps added, the relatively small amount of extra compute for fine-tuning, and the improved, direct answer to the pope question." data-tts-id="tts_46_95cdb455">
       <table class="caption-top" style="width:100%;">
        <colgroup>
         <col style="width: 11%"/>
         <col style="width: 17%"/>
         <col style="width: 15%"/>
         <col style="width: 8%"/>
         <col style="width: 7%"/>
         <col style="width: 32%"/>
         <col style="width: 8%"/>
        </colgroup>
        <thead>
         <tr class="header">
          <th>
           Model (year)
          </th>
          <th>
           Incremental training steps
          </th>
          <th>
           Corpus size (tokens &asymp; &ldquo;Bibles&rdquo;*)
          </th>
          <th>
           Max context (tokens)
          </th>
          <th>
           Rough train compute
          </th>
          <th>
           Typical answer to &ldquo;Who is the current pope? (3 May 2025)&rdquo;
          </th>
          <th>
           Typical inference HW
          </th>
         </tr>
        </thead>
        <tbody>
         <tr class="odd">
          <td>
           <strong>
            GPT-3.5 / ChatGPT (2022)
           </strong>
          </td>
          <td>
           + Supervised instruction fine-tune + RLHF
          </td>
          <td>
           &asymp; 500 B + few &times; 10‚Åµ labelled prompts
          </td>
          <td>
           4 096 &uarr;
          </td>
          <td>
           + a few V100-days &uarr;
          </td>
          <td>
           &ldquo;Pope Francis.&rdquo; (Model follows instruction but is
           <strong>
            stale
           </strong>
           ; unaware of Apr 2025 death)
          </td>
          <td>
           cloud GPU
          </td>
         </tr>
        </tbody>
       </table>
       <audio data-autoplay="">
        <source src="media/audio/Heres_the_table_entr_5d5a4552-a6bf-4674-b366-72ff487650fb.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="16271" data-tts="However, a crucial limitation emerged: the model's knowledge is frozen at the time its pre-training data was collected. It follows instructions well, but its information can be outdated. It wouldn't know about hypothetical events after its knowledge cutoff." data-tts-id="tts_47_e0c52493">
       <ul>
        <li>
         <strong>
          Limitation:
         </strong>
         Knowledge is frozen at its pre-training data cutoff date &rArr;
         <strong>
          Stale
         </strong>
         information.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/However_a_crucial_li_5742c69c-a2a6-4002-982a-0f5c5945b2d1.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="11808" data-tts="Let's see this in action. A GPT-3.5 model would likely correctly identify Pope Francis based on its training data, but be unaware of any hypothetical later events." data-tts-id="tts_48_abb09900">
       <p>
        <em>
         (Note: Live demo of GPT-3.5 answer, highlighting helpfulness but staleness.)
        </em>
       </p>
       <audio data-autoplay="">
        <source src="media/audio/Lets_see_this_in_act_419fedb9-ca55-46aa-9f57-14148bbdf5b9.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="gpt-4o-gpt-4-turbo-augmenting-with-tools-12">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       GPT-4o / GPT-4 Turbo: Augmenting with Tools (1/2)
      </h2>
      <div class="fragment" data-autoslide="10008" data-tts="We saw that the alignment step made models helpful, but their knowledge remained static. And the fine-tuning data itself was relatively small compared to pre-training." data-tts-id="tts_49_498e15d6">
       <ul>
        <li>
         Problem: Aligned models are helpful but have
         <strong>
          stale
         </strong>
         knowledge. Fine-tuning adds behavior, not much new knowledge.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/We_saw_that_the_alig_a13d08f0-0446-4fc0-be1a-e1037d4b4e86.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="13140" data-tts="How can a model access up-to-date information or perform tasks beyond its internal knowledge, like calculations or running code? The next major innovation was enabling models to use external 'tools'." data-tts-id="tts_50_458dc5bb">
       <ul>
        <li>
         <strong>
          Key Innovation:
         </strong>
         Fine-tuning the model to use external &ldquo;tools&rdquo; via function calling / plugins.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/How_can_a_model_acce_10f8bf84-9b8a-46ad-8152-3b8f4223b2ba.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="17820" data-tts="Models like GPT-4, and later variants like Turbo and -o, were fine-tuned specifically to recognize when a user's request requires external help. This could be searching the web, running a piece of Python code in a sandbox, or calling other defined functions." data-tts-id="tts_51_d7e6c510">
       <pre><code>*   Model learns to recognize when a task requires external info (web search) or computation (running code).</code></pre>
       <audio data-autoplay="">
        <source src="media/audio/Models_like_GPT4_and_72c6f7b8-6936-41a0-b1c9-2923f56bc435.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="gpt-4o-gpt-4-turbo-augmenting-with-tools-22">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       GPT-4o / GPT-4 Turbo: Augmenting with Tools (2/2)
      </h2>
      <div class="fragment" data-autoslide="14148" data-tts="When the model detects such a need, it doesn't try to hallucinate an answer. Instead, it generates a structured request to the appropriate tool &ndash; for example, formulating a search query like `search('current pope')`." data-tts-id="tts_52_81430e63">
       <pre><code>*   It generates a structured request (e.g., `search("current pope")`), receives the tool's output, and synthesizes the final answer.</code></pre>
       <audio data-autoplay="">
        <source src="media/audio/When_the_model_detec_6ee5761f-47ff-41cd-b797-e95380b8a22b.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="13500" data-tts="The external tool (like a search engine API or a code execution environment) runs the request, returns the result, and the LLM incorporates this external information into its final response to the user." data-tts-id="tts_53_75259f72">
       <ul>
        <li>
         <strong>
          Result:
         </strong>
         Overcomes stale knowledge; improves capabilities (calculation, real-time data). Introduces new failure modes (choosing wrong tool, tool returning bad data).
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/The_external_tool_li_84701c30-e2e2-4794-9e6e-84c2adb45f41.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="19080" data-tts="Let's look at the table row. Note the further scaling in data and context, the massive increase in estimated training compute for the base GPT-4 model, and the crucial addition of 'Tool-use fine-tune'. The answer to our pope question is now potentially up-to-date, citing external sources." data-tts-id="tts_54_aa20079c">
       <table class="caption-top" style="width:100%;">
        <colgroup>
         <col style="width: 12%"/>
         <col style="width: 23%"/>
         <col style="width: 12%"/>
         <col style="width: 7%"/>
         <col style="width: 7%"/>
         <col style="width: 26%"/>
         <col style="width: 9%"/>
        </colgroup>
        <thead>
         <tr class="header">
          <th>
           Model (year)
          </th>
          <th>
           Incremental training steps
          </th>
          <th>
           Corpus size (tokens &asymp; &ldquo;Bibles&rdquo;*)
          </th>
          <th>
           Max context (tokens)
          </th>
          <th>
           Rough train compute
          </th>
          <th>
           Typical answer to &ldquo;Who is the current pope? (3 May 2025)&rdquo;
          </th>
          <th>
           Typical inference HW
          </th>
         </tr>
        </thead>
        <tbody>
         <tr class="odd">
          <td>
           <strong>
            GPT-4o / GPT-4 Turbo (23-24)
           </strong>
          </td>
          <td>
           + Tool-use fine-tune (plugins: Browse, Code Interpreter)
          </td>
          <td>
           &asymp; 1 T &asymp; 1.6 M Bibles &uarr;
          </td>
          <td>
           8 K / 128 K &uarr;
          </td>
          <td>
           &asymp; 2.5 M A100-days &uarr;
          </td>
          <td>
           &ldquo;Pope Francis died &hellip; Pope John XXIV was elected&hellip;&rdquo; (cites news)
          </td>
          <td>
           cloud GPU + API calls
          </td>
         </tr>
        </tbody>
       </table>
       <audio data-autoplay="">
        <source src="media/audio/Lets_look_at_the_tab_d5bbce66-5288-42c7-b666-158a6e14caf2.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="9540" data-tts="Now, if we try our pope question with a tool-enabled model like GPT-4o, it should ideally use its browsing tool to find the latest information." data-tts-id="tts_55_0f034515">
       <p>
        <em>
         (Note: Live demo of GPT-4o with browsing for the pope question.)
        </em>
       </p>
       <audio data-autoplay="">
        <source src="media/audio/Now_if_we_try_our_po_72a705f5-464d-45f5-a292-1a11f5d1a9ba.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="gpt-4o1-optimizing-for-reasoning-12">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       GPT-4o1: Optimizing for Reasoning (1/2)
      </h2>
      <div class="fragment" data-autoslide="12348" data-tts="Even with tools, complex problems requiring multiple steps of logical deduction remained challenging. Models could sometimes 'guess' the right answer for the wrong reasons, or fail on intricate tasks." data-tts-id="tts_56_01b88c02">
       <ul>
        <li>
         Problem: Models still struggle with multi-step reasoning and complex problem-solving.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Even_with_tools_comp_49583b0d-f08d-4709-9329-b6b2e8d45600.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="13644" data-tts="The next step, exemplified by models like GPT-4o1 (o for omni, 1 for reasoning focus), was to explicitly optimize the model's *reasoning process* itself, often post-training." data-tts-id="tts_57_93617369">
       <ul>
        <li>
         <strong>
          Key Innovation:
         </strong>
         Explicitly training the model (post-training) to perform and verify step-by-step reasoning (&ldquo;Chain of Thought&rdquo; - CoT).
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/The_next_step_exempl_c0e98fca-db8a-4c59-89aa-685061977325.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="13896" data-tts="This often involves techniques related to 'Chain of Thought' prompting, where the model is encouraged or directly trained (using reinforcement learning) to generate intermediate reasoning steps *before* arriving at the final answer." data-tts-id="tts_58_2c69773e">
       <pre><code>*   Uses RL to reward models that generate a logical intermediate thought process before giving the final answer.
*   Focuses on improving performance on complex tasks requiring multi-step logic (math, coding, science).</code></pre>
       <audio data-autoplay="">
        <source src="media/audio/This_often_involves__8b4cfdf6-3387-4405-a255-61260eb803c5.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="gpt-4o1-optimizing-for-reasoning-22">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       GPT-4o1: Optimizing for Reasoning (2/2)
      </h2>
      <div class="fragment" data-autoslide="11916" data-tts="Think of it like forcing students to 'show their work' on a math problem. Rewarding the correct logical steps, not just the final answer, helps the model develop more robust reasoning pathways." data-tts-id="tts_59_bb6369c7">
       <audio data-autoplay="">
        <source src="media/audio/Think_of_it_like_for_8a5d7569-5c0c-4999-bc03-9019039e18aa.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <aside class="notes">
       <p>
        Analogy: Forcing students to &ldquo;show their work&rdquo;. Reduces likelihood of confident guessing on complex problems. Not foolproof - the reasoning chain can be flawed but internally consistent.
       </p>
       <style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }
       </style>
      </aside>
      <div class="fragment" data-autoslide="14292" data-tts="The result is significantly better performance on benchmarks measuring reasoning abilities. While not foolproof &ndash; the reasoning chain itself can sometimes be flawed &ndash; it tends to reduce confident errors on complex tasks." data-tts-id="tts_60_be47c186">
       <ul>
        <li>
         <strong>
          Result:
         </strong>
         Significantly better benchmark scores on reasoning tasks. Reduced hallucination
         <em>
          on the reasoning path itself
         </em>
         , though errors can still occur. Slower inference due to generating intermediate steps.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/The_result_is_signif_a8ccffb0-f1f6-4bd7-b17d-f0db8eb7fe2d.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="23112" data-tts="Looking at our table, GPT-4o1 builds on GPT-4o, using roughly the same massive dataset but adding a specific post-training phase focused on reasoning reinforcement. The compute for this phase is significant, though less than the initial pre-training. The hypothetical output now includes not just the facts but the justification." data-tts-id="tts_61_7f8db8d5">
       <table class="caption-top">
        <colgroup>
         <col style="width: 12%"/>
         <col style="width: 19%"/>
         <col style="width: 12%"/>
         <col style="width: 8%"/>
         <col style="width: 9%"/>
         <col style="width: 30%"/>
         <col style="width: 8%"/>
        </colgroup>
        <thead>
         <tr class="header">
          <th>
           Model (year)
          </th>
          <th>
           Incremental training steps
          </th>
          <th>
           Corpus size (tokens &asymp; &ldquo;Bibles&rdquo;*)
          </th>
          <th>
           Max context (tokens)
          </th>
          <th>
           Rough train compute
          </th>
          <th>
           Typical answer to &ldquo;Who is the current pope? (3 May 2025)&rdquo;
          </th>
          <th>
           Typical inference HW
          </th>
         </tr>
        </thead>
        <tbody>
         <tr class="odd">
          <td>
           <strong>
            GPT-4o1 (reasoning, 24-25)
           </strong>
          </td>
          <td>
           + Chain-of-thought reinforcement (post-training)
          </td>
          <td>
           same &asymp; 1 T
          </td>
          <td>
           8 K &ndash; 128 K
          </td>
          <td>
           + &asymp; 10&ndash;20 k A100-days &uarr;
          </td>
          <td>
           Gives same facts
           <strong>
            and
           </strong>
           step-by-step justification (citing sources).
          </td>
          <td>
           cloud GPU + tools
          </td>
         </tr>
        </tbody>
       </table>
       <audio data-autoplay="">
        <source src="media/audio/Looking_at_our_table_5bea1f19-31c8-474e-8e25-4a7b5e48c992.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="17568" data-tts="If available, a demo of GPT-4o1 on the pope question might show it explicitly stating: 'Searching for current pope...' then 'Information indicates Pope Francis died...' then 'Confirming successorÈÄâ‰∏æ...' leading to the final answer." data-tts-id="tts_62_7280daa5">
       <p>
        <em>
         (Note: Live demo of GPT-4o1 showing reasoning steps for the pope question, assuming available.)
        </em>
       </p>
       <audio data-autoplay="">
        <source src="media/audio/If_available_a_demo__967cf2cc-1414-48fd-b82d-f16f5efdf105.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="analogy-learning-like-a-model">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Analogy: Learning like a model
      </h2>
      <div class="fragment" data-autoslide="4716" data-tts="Let's try an analogy to summarize the GPT evolution in terms of learning:" data-tts-id="tts_63_d2dc2055">
       <audio data-autoplay="">
        <source src="media/audio/Lets_try_an_analogy__fe3ce922-d094-40d6-808d-ee9e7ce683d1.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="11412" data-tts="**GPT-3 (Base Model):** Imagine reading HUNDREDS OF THOUSANDS of textbooks and websites. You become incredibly fluent and knowledgeable about the text you've seen." data-tts-id="tts_64_b977f0c1">
       <ul>
        <li>
         <strong>
          GPT-3:
         </strong>
         Read the textbook (a massive one!).
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/GPT3_Base_Model_Imag_16349963-3946-4718-a8c3-6047d074491a.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="15804" data-tts="**GPT-3.5 / ChatGPT:** Now, on top of that, you focus intensely on textbook examples that have *solutions* provided. You learn the *format* of answering questions correctly and following instructions." data-tts-id="tts_65_177a945c">
       <ul>
        <li>
         <strong>
          GPT-3.5:
         </strong>
         Read more textbooks + focus on examples with solutions &rArr; know how to answer questions.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/GPT35__ChatGPT_Now_o_9cfb60da-dd03-435b-8972-a5c43ceda637.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-tts="**GPT-4o / Turbo:** You read even *more* textbooks, and crucially, you learn how to use tools like a calculator, the internet, and maybe a programming environment to help you answer questions you couldn't solve just from memory." data-tts-id="tts_66_e93e4d0b">
       <ul>
        <li>
         <strong>
          GPT-4:
         </strong>
         Read a LOT more! + Learn to use a calculator, the internet etc. to improve answers.
        </li>
       </ul>
      </div>
      <div class="fragment" data-autoslide="19368" data-tts="**GPT-4o1:** Finally, you specifically practice solving complex problems, step-by-step. You get feedback not just on your final answer, but on whether your *reasoning path* was logical. You update how you think based on which lines of reasoning lead to correct solutions." data-tts-id="tts_67_8dac7c2d">
       <ul>
        <li>
         <strong>
          o1:
         </strong>
         Read practice questions, try solving them step-by-step, get feedback on the
         <em>
          reasoning
         </em>
         , update thinking based on successful paths.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/GPT4o1_Finally_you_s_b3ffc3cf-eb9f-4e5e-9a80-f5991aed7830.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="one-page-lineage-table-summary">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       One-page lineage table (Summary)
      </h2>
      <div class="fragment" data-autoslide="5256" data-tts="Let's revisit our roadmap, the lineage table, now that we've walked through each step." data-tts-id="tts_68_88d3a35c">
       <p>
        <em>
         Let&rsquo;s revisit the full progression.
        </em>
       </p>
       <audio data-autoplay="">
        <source src="media/audio/Lets_revisit_our_roa_b078596c-6fe5-4663-8e01-e17180b77e75.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="36900" data-tts="We started with N-grams, simple statistical models with tiny context. Then came the Transformer, enabling massive scaling in data and parameters (GPT-1 to 3), leading to fluent but unaligned models. Alignment (SFT + RLHF) made them conversational but stale (GPT-3.5). Adding tools overcame staleness (GPT-4o/Turbo). And finally, optimizing the reasoning process improved reliability on complex tasks (GPT-4o1)." data-tts-id="tts_69_92647463">
       <table class="caption-top">
        <colgroup>
         <col style="width: 10%"/>
         <col style="width: 21%"/>
         <col style="width: 12%"/>
         <col style="width: 6%"/>
         <col style="width: 9%"/>
         <col style="width: 30%"/>
         <col style="width: 7%"/>
        </colgroup>
        <thead>
         <tr class="header">
          <th>
           Model (year)
          </th>
          <th>
           Incremental training steps
          </th>
          <th>
           Corpus size (tokens &asymp; &ldquo;Bibles&rdquo;*)
          </th>
          <th>
           Max context (tokens)
          </th>
          <th>
           Rough train compute
          </th>
          <th>
           Typical answer to &ldquo;Who is the current pope? (3 May 2025)&rdquo;
          </th>
          <th>
           Typical inference HW
          </th>
         </tr>
        </thead>
        <tbody>
         <tr class="odd">
          <td>
           <strong>
            4-gram (Brown, 1960s)
           </strong>
          </td>
          <td>
           Count 4-grams, MLE
          </td>
          <td>
           1 M &asymp; 1.3 Bibles
          </td>
          <td>
           3 (last three words)
          </td>
          <td>
           &lt; 1 CPU-h
          </td>
          <td>
           &ldquo;the pope is the &hellip;&rdquo; (nonsense)
          </td>
          <td>
           any laptop CPU
          </td>
         </tr>
         <tr class="even">
          <td>
           <strong>
            GPT-1 &rarr; GPT-3 (2018-20)
           </strong>
          </td>
          <td>
           Same next-token objective,
           <strong>
            massive scale-up
           </strong>
           of params &amp; data
          </td>
          <td>
           0.8 B &rarr; 500 B &asymp; 1 K &rarr; 640 K Bibles &uarr;
          </td>
          <td>
           512 &rarr; 2 048 &uarr;
          </td>
          <td>
           &asymp;3 &times; 10&sup2;&sup3; FLOP &asymp; 355 V100-years &uarr;
          </td>
          <td>
           &ldquo;Read our new article on the pope here.&rdquo; (fluent but ignores the question&rsquo;s intent)
          </td>
          <td>
           single GPU &rarr; multi-GPU
          </td>
         </tr>
         <tr class="odd">
          <td>
           <strong>
            GPT-3.5 / ChatGPT (2022)
           </strong>
          </td>
          <td>
           + Supervised instruction fine-tune + RLHF
          </td>
          <td>
           &asymp; 500 B + few &times; 10‚Åµ labelled prompts
          </td>
          <td>
           4 096 &uarr;
          </td>
          <td>
           + a few V100-days &uarr;
          </td>
          <td>
           &ldquo;Pope Francis.&rdquo; (Model follows instruction but is
           <strong>
            stale
           </strong>
           ; unaware of April 2025 death.)
          </td>
          <td>
           cloud GPU
          </td>
         </tr>
         <tr class="even">
          <td>
           <strong>
            GPT-4o / GPT-4 Turbo (23-24)
           </strong>
          </td>
          <td>
           + Tool-use fine-tune (plugins: Browse, Code Interpreter)
          </td>
          <td>
           &asymp; 1 T &asymp; 1.6 M Bibles &uarr;
          </td>
          <td>
           8 K / 128 K &uarr;
          </td>
          <td>
           &asymp; 2.5 M A100-days &uarr;
          </td>
          <td>
           &ldquo;Pope Francis died on 14 Apr 2025; Pope John XXIV was elected on 28 Apr 2025.&rdquo; (cites news)
          </td>
          <td>
           cloud GPU + API calls
          </td>
         </tr>
         <tr class="odd">
          <td>
           <strong>
            GPT-4o1 (reasoning, 24-25)
           </strong>
          </td>
          <td>
           + Chain-of-thought reinforcement (post-training)
          </td>
          <td>
           same &asymp; 1 T
          </td>
          <td>
           8 K &ndash; 128 K
          </td>
          <td>
           + &asymp; 10&ndash;20 k A100-days &uarr;
          </td>
          <td>
           Gives same facts
           <strong>
            and
           </strong>
           step-by-step justification (citing sources).
          </td>
          <td>
           cloud GPU + tools
          </td>
         </tr>
        </tbody>
       </table>
       <audio data-autoplay="">
        <source src="media/audio/We_started_with_Ngra_28296635-a487-482d-99ee-7fc19eab6e45.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="9216" data-tts="Each incremental step built upon the previous ones, adding new capabilities but also inheriting or modifying existing limitations." data-tts-id="tts_70_ce83bfe2">
       <p>
        <em>
         Bible rough equivalence uses &asymp; 780,000 words per King James Bible. Pope example is hypothetical.
        </em>
       </p>
       <audio data-autoplay="">
        <source src="media/audio/Each_incremental_ste_5ed0f96c-f2b7-488c-92da-bf8f429eea90.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="where-are-things-going-trends">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Where are things going? (Trends)
      </h2>
      <div class="fragment" data-autoslide="3672" data-tts="Based on this history, what future trends might we expect?" data-tts-id="tts_71_8b742460">
       <audio data-autoplay="">
        <source src="media/audio/Based_on_this_histor_bb475b85-51b0-4c22-b71d-c0b8cd64975e.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="27864" data-tts="First, continued rapid evolution seems likely. The pace from GPT-1 in 2018 to GPT-4o1 in 2024 is breathtaking. Expect significant, possibly unpredictable, advances. This could involve true multi-modality beyond text and images, more autonomous 'agentic' systems, or even entirely new AI architectures moving beyond the Transformer." data-tts-id="tts_72_e255d164">
       <ul>
        <li>
         <strong>
          Continued Rapid Evolution:
         </strong>
         Pace suggests increasing capability&hellip; significant, perhaps unpredictable, changes ahead (true multi-modality, agentic systems, new architectures?).
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/First_continued_rapi_caee3666-5fe2-4052-80a6-c7554edc2cf0.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="20160" data-tts="Second, context windows will likely keep expanding. We saw the trend from 512 tokens to 128,000 or more. Expect models capable of natively handling much larger documents, entire codebases, or very long conversations without needing complex summarization tricks." data-tts-id="tts_73_c2cfdc99">
       <ul>
        <li>
         <strong>
          Context Windows Expanding:
         </strong>
         Following the trend&hellip; Expect models handling larger documents/conversations natively.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Second_context_windo_ed862464-0d07-40d5-972f-cfdfa4b701e7.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="where-are-things-going-limitations">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Where are things going? (Limitations)
      </h2>
      <div class="fragment" data-autoslide="7164" data-tts="However, some fundamental limitations rooted in the current paradigm will likely persist, at least partially." data-tts-id="tts_74_6203c93a">
       <audio data-autoplay="">
        <source src="media/audio/However_some_fundame_2b10ebb7-7819-47ec-ac0a-7d946a317904.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="15876" data-tts="Bias remains a challenge. Since models are trained on vast internet datasets, they inevitably reflect the biases present in that data. Alignment helps mitigate harmful outputs, but subtle biases are hard to eliminate entirely." data-tts-id="tts_75_b4da7895">
       <ul>
        <li>
         <strong>
          Bias
         </strong>
         : Trained on the internet. Reflects whatever it is trained on. Alignment helps, but doesn&rsquo;t fully solve.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Bias_remains_a_chall_74b88e7e-d6aa-4270-a5a6-00233cc36161.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="13824" data-tts="Stale Knowledge is inherent to pre-training. Even with tools like web search, the model's *core understanding* is based on its cutoff date. You need to ensure tools are being used appropriately if real-time info is critical." data-tts-id="tts_76_2cc1e7cf">
       <ul>
        <li>
         <strong>
          Stale Knowledge
         </strong>
         : Core knowledge is fixed at pre-training. Tools (like search) are essential Band-Aids for current info. Need to verify tool use.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Stale_Knowledge_is_i_8538c2c9-e3ef-4fc2-a50d-c545fcc2d719.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="18792" data-tts="Compute Cost and Energy use are direct results of the 'scale is all you need' approach. Training and running state-of-the-art models requires massive data centers. This makes powerful AI difficult to run locally, raising privacy and access concerns tied to cloud dependence." data-tts-id="tts_77_9365b17f">
       <ul>
        <li>
         <strong>
          Compute Cost / Energy
         </strong>
         : Scaling requires huge resources. Hard to run locally &rArr; privacy/access issues persist.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Compute_Cost_and_Ene_e59db11d-0832-4b92-9707-e7b4bcfa9173.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="fragment" data-autoslide="18972" data-tts="Context Window Limits, while expanding, still exist. A model processing 128k tokens is impressive, but human experience involves vastly more context accumulated over years. True human-like reasoning or long-term memory in agents remains a distant goal." data-tts-id="tts_78_ba77c808">
       <ul>
        <li>
         <strong>
          Context Window Limits
         </strong>
         : Expanding, but still finite. How much context does a human process in a year? Models are far from that level of integrated experience.
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Context_Window_Limit_f068d355-f057-48c9-a64a-850e2e04d440.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="references-12">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       References (1/2)
      </h2>
      <div class="fragment" data-autoslide="4716" data-tts="Here are some key references mentioned throughout the presentation for further reading." data-tts-id="tts_79_85bdc77e">
       <ul>
        <li>
         <strong>
          Brown Corpus:
         </strong>
         <a href="https://en.wikipedia.org/wiki/Brown_Corpus">
          Wikipedia
         </a>
         (Example N-gram corpus)
        </li>
        <li>
         <strong>
          Transformer:
         </strong>
         Vaswani, A., et al.&nbsp;(2017). Attention is All You Need.
         <a href="https://arxiv.org/abs/1706.03762">
          arXiv:1706.03762
         </a>
        </li>
        <li>
         <strong>
          GPT-3 Scale/Compute:
         </strong>
         <a href="https://lambdalabs.com/blog/demystifying-gpt-3">
          Lambda Labs Blog (GPT-3 Demystified)
         </a>
         (Good overview of scale)
        </li>
        <li>
         <strong>
          Instruction-tuning / RLHF:
         </strong>
         Ouyang, L., et al.&nbsp;(2022). Training language models to follow instructions with human feedback.
         <a href="https://arxiv.org/abs/2203.02155">
          arXiv:2203.02155
         </a>
         (The InstructGPT paper, key to ChatGPT)
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/Here_are_some_key_re_07213460-4251-4142-a174-bfd56fbc451b.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="references-22">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       References (2/2)
      </h2>
      <div class="fragment" data-autoslide="2664" data-tts="And continuing with the references..." data-tts-id="tts_80_73ed6fef">
       <ul>
        <li>
         <strong>
          Tool Use (Plugins/Functions):
         </strong>
         <a href="https://openai.com/index/chatgpt-plugins/">
          OpenAI Blog (Plugins)
         </a>
         ,
         <a href="https://openai.com/index/new-models-and-developer-products-announced-at-devday/">
          OpenAI Blog (DevDay - Functions/Tools)
         </a>
        </li>
        <li>
         <strong>
          GPT-4 Compute Estimates:
         </strong>
         Patel, A. B., et al.&nbsp;(2025). SpecInF&hellip;
         <a href="https://arxiv.org/html/2503.02550v3">
          arXiv:2503.02550v3
         </a>
         (Note: Real compute figures are often secret; estimates vary widely)
        </li>
        <li>
         <strong>
          GPT-4o1 Reasoning:
         </strong>
         <a href="https://openai.com/index/introducing-openai-o1-preview/">
          OpenAI Blog (o1-preview)
         </a>
        </li>
        <li>
         <strong>
          (Optional) Chain of Thought Concept:
         </strong>
         Wei, J., et al.&nbsp;(2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.
         <a href="https://arxiv.org/abs/2201.11903">
          arXiv:2201.11903
         </a>
         (Influential paper on the reasoning technique)
        </li>
        <li>
         <strong>
          Hypothetical News Examples:
         </strong>
         <a href="https://www.reuters.com/world/pope-francis-has-died-vatican-says-video-statement-2025-04-21/">
          Reuters (Example)
         </a>
         ,
         <a href="https://www.vaticannews.va/en/vatican-city/news/2025-04/conclave-elect-new-pope-cardinals-beginning-date-may-2025.html">
          Vatican News (Example)
         </a>
         (Used for the pope scenario)
        </li>
       </ul>
       <audio data-autoplay="">
        <source src="media/audio/And_continuing_with__110859f4-4a1b-4ea6-a25e-bbc4e342ac4c.mp3" type="audio/mpeg"/>
       </audio>
      </div>
     </section>
     <section class="slide level2" data-autoslide="100" id="thank-you">
      <div class="fragment" data-autoslide="10">
      </div>
      <h2>
       Thank You
      </h2>
      <div class="fragment" data-autoslide="12204" data-tts="That concludes our brief history of language models. I hope this provides a useful mental model for understanding these powerful and rapidly evolving tools. Any questions?" data-tts-id="tts_81_9e422519">
       <p>
        Questions?
       </p>
       <audio data-autoplay="">
        <source src="media/audio/That_concludes_our_b_fd85b013-a2b9-4957-af26-4b8af25f84c5.mp3" type="audio/mpeg"/>
       </audio>
      </div>
      <div class="quarto-auto-generated-content">
       <div class="footer footer-default">
       </div>
      </div>
     </section>
    </section>
   </div>
  </div>
  <script>
   window.backupDefine = window.define; window.define = undefined;
  </script>
  <script src="presentation_files/libs/revealjs/dist/reveal.js">
  </script>
  <!-- reveal.js plugins -->
  <script src="presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js">
  </script>
  <script src="presentation_files/libs/revealjs/plugin/pdf-export/pdfexport.js">
  </script>
  <script src="presentation_files/libs/revealjs/plugin/reveal-menu/menu.js">
  </script>
  <script src="presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js">
  </script>
  <script src="presentation_files/libs/revealjs/plugin/quarto-support/support.js">
  </script>
  <script src="presentation_files/libs/revealjs/plugin/notes/notes.js">
  </script>
  <script src="presentation_files/libs/revealjs/plugin/search/search.js">
  </script>
  <script src="presentation_files/libs/revealjs/plugin/zoom/zoom.js">
  </script>
  <script src="presentation_files/libs/revealjs/plugin/math/math.js">
  </script>
  <script>
   window.define = window.backupDefine; window.backupDefine = undefined;
  </script>
  <script>
   // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 100,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
  </script>
  <script id="quarto-html-after-body" type="application/javascript">
   window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
  </script>
  <script>
   document.addEventListener("DOMContentLoaded", function () {
      const startButton = document.getElementById("startPresentationButton");

      startButton.addEventListener("click", () => {
        // Advance to the next slide
        Reveal.next();
      });
    });
  </script>
  <script>
   // Playback speed feature
    document.addEventListener("DOMContentLoaded", function () {
      const speedButton = document.getElementById("speedButton");
      const playbackRates = [1, 1.5, 2, 0.75];
      let currentRateIndex = 0;

 function applyPlaybackRate() {
  const currentRate = playbackRates[currentRateIndex];

  // Update media playback rates
  document.querySelectorAll("audio, video").forEach(media => {
    media.playbackRate = currentRate;
  });

  // Update slide durations
  document.querySelectorAll(".fragment[data-autoslide]").forEach(fragment => {
    const originalDuration = parseInt(fragment.getAttribute("data-original-autoslide") || fragment.getAttribute("data-autoslide"));
    // Store original duration if not already stored
    if (!fragment.getAttribute("data-original-autoslide")) {
      fragment.setAttribute("data-original-autoslide", originalDuration);
    }
    // Adjust duration based on playback rate
    fragment.setAttribute("data-autoslide", Math.round(originalDuration * (1 / currentRate)));
  });
}

      speedButton.addEventListener("click", () => {
        currentRateIndex = (currentRateIndex + 1) % playbackRates.length;
        speedButton.textContent = playbackRates[currentRateIndex] + "x";
        applyPlaybackRate();
      });

      Reveal.addEventListener('slidechanged', applyPlaybackRate);
      applyPlaybackRate();
    });

    // Volume control feature
    document.addEventListener("DOMContentLoaded", function () {
      const volumeSlider = document.getElementById("volumeSlider");

      function updateVolumes() {
        document.querySelectorAll("audio, video").forEach(media => {
          media.volume = volumeSlider.value;
        });
      }

      volumeSlider.addEventListener("input", updateVolumes);
    });
  </script>
  <button id="speedButton" style="position: fixed; z-index: 1000; bottom: 20px; left: 50%; transform: translateX(-50%); margin-left: 100px;">
   1x
  </button>
  <input id="volumeSlider" max="1" min="0" step="0.01" style="position: fixed; z-index: 1000; bottom: 50px; left: 50%; transform: translateX(-50%); width: 200px;" type="range" value="1"/>
 </body>
</html>
